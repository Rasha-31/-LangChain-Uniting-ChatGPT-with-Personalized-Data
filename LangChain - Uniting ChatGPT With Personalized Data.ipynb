{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "66e921b9",
   "metadata": {},
   "source": [
    "# How to connect ChatGPT with your own data?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ce13ef0",
   "metadata": {},
   "source": [
    "# ChatGPT using Langchain by extracting data from a text Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "737ad4d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1a776a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "390c71e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\rasha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain import OpenAI, VectorDBQA\n",
    "from langchain.document_loaders import DirectoryLoader\n",
    "import magic\n",
    "import os\n",
    "import nltk\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"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\"\n",
    "\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2afae6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import UnstructuredFileLoader\n",
    "\n",
    "def file_loader(path):\n",
    "    return UnstructuredFileLoader(path)\n",
    "\n",
    "# loader = file_loader(\"./LangChain-Documentation.txt\") \n",
    "loader = file_loader(input(\"Enter the path of your text file:\"))\n",
    "\n",
    "documents = loader.load()\n",
    "\n",
    "\n",
    "\n",
    "text_splitter = CharacterTextSplitter(chunk_size=2000, chunk_overlap=100)\n",
    "texts = text_splitter.split_documents(documents)\n",
    "embeddings = OpenAIEmbeddings(openai_api_key='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')\n",
    "docsearch = Chroma.from_documents(texts, embeddings)\n",
    "\n",
    "from langchain.chains import RetrievalQA\n",
    "qa = VectorDBQA.from_chain_type(llm=OpenAI(), chain_type=\"stuff\", vectorstore=docsearch)\n",
    "\n",
    "documents\n",
    "\n",
    "query1 = input(\"Enter the Question you want to ask from your model: \")\n",
    "qa.run(query1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3c9c647",
   "metadata": {},
   "source": [
    "# URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "04353e23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain import PromptTemplate\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "\n",
    "# Twitter\n",
    "import tweepy\n",
    "\n",
    "#Scraping\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from markdownify import  markdownify as md\n",
    "\n",
    "# YouTube\n",
    "from langchain.document_loaders import YoutubeLoader\n",
    "\n",
    "#Environment Variables\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "980a1c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "OPENAI_API_KEY = os.getenv('OPENAI_API_KEY', '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')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b1675106",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pull_from_website (url):\n",
    "    \n",
    "    #Doing a try in case it doesn't work\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "    except:\n",
    "        # In case it doesn't work\n",
    "        print (\"Whoops, error\")\n",
    "        return\n",
    "    \n",
    "    #Put your response in a beautiful soup\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    # Get your text\n",
    "    \n",
    "    text = soup.get_text()\n",
    "    # Convert your html to markdown. This reduces tokens and noise\n",
    "    text = md(text)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9ce24b06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: urllib3 in c:\\users\\rasha\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (2.0.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install urllib3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "73b92cee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the number of urls you want to input: 2\n",
      "Enter string 1: https://eladgil.com/\n",
      "Enter string 2: https://blog.eladgil.com/p/defensibility-and-competition\n"
     ]
    }
   ],
   "source": [
    "website_data = \"\"\n",
    "n = int(input(\"Enter the number of urls you want to input: \"))\n",
    "\n",
    "urls = [input(f\"Enter string {i + 1}: \") for i in range(n)]\n",
    "\n",
    "for url in urls:\n",
    "    text = pull_from_website (url)\n",
    "\n",
    "    website_data += text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "02472828",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Elad Gil\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Welcome to Elad Gil's retro homepage!\n",
      "\n",
      " Who? I am a technology entrepreneur. LinkedIn profile is here.\n",
      "What?\n",
      "I am an investor or advisor to companies including Airbnb, Airtable, Anduril, Brex, Checkr, Coinbase, dbt Labs, Deel, Figma, Flexport, Gitlab, Gusto, Instacart, Navan, Notion, Opendoor, PagerDuty, Pinterest, Retool, Rippling, Samsara, Square, Stripe\n",
      "I am involved with AI com\n"
     ]
    }
   ],
   "source": [
    "print(website_data[:400])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "534f7e97",
   "metadata": {},
   "source": [
    "# Pulling data from Youtube"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a826dade",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain.document_loaders import YoutubeLoader\n",
    "llm = OpenAI(temperature=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b426d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import UnstructuredFileLoader\n",
    "\n",
    "def file_loader(path):\n",
    "    return UnstructuredFileLoader(path)\n",
    "\n",
    "# loader = file_loader(\"./LangChain-Documentation.txt\") \n",
    "loader = file_loader(input(\"Enter the path of your text file:\"))\n",
    "\n",
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "6861c33c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter your yt url: pNcQ5XXMgH4\n"
     ]
    }
   ],
   "source": [
    "def from_youtube_url(path):\n",
    "    return YoutubeLoader(path)\n",
    "\n",
    "loader = from_youtube_url(input(\"Enter your yt url: \"))\n",
    "\n",
    "result = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d2b05d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = YoutubeLoader.from_youtube_url(\"https://www.youtube.com/watch?v=pNcQ5XXMgH4\", add_video_info=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "46ee56e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "e9ee6749",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "282c044a",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter (chunk_size=2000, chunk_overlap=100)\n",
    "texts = text_splitter.split_documents (result)\n",
    "#chain = load_summarize_chain (llm, chain_type=\"map_reduce\", verbose=False)\n",
    "#chain.run(texts[:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "5e6af769",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7b839955",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='See discussions, st ats, and author pr ofiles f or this public ation at : https://www .researchgate.ne t/public ation/277941134\\nComputer science research: the top 100 institutions in India and in the world\\nArticle \\xa0\\xa0 in\\xa0\\xa0Scient ome trics  · August 2015\\nDOI: 10.1007/s11192-015-1612-8\\nCITATIONS\\n44READS\\n10,015\\n3 author s:\\nSome o f the author s of this public ation ar e also w orking on these r elat ed pr ojects:\\nScient o-text View pr oject\\nDocument L evel Sentiment Analysis  View pr oject\\nVivek K umar Singh\\nBanar as Hindu Univ ersity\\n202 PUBLICA TIONS \\xa0\\xa0\\xa02,459  CITATIONS \\xa0\\xa0\\xa0\\nSEE PROFILE\\nAshr af Uddin\\nSouth Asian Univ ersity\\n34 PUBLICA TIONS \\xa0\\xa0\\xa0732 CITATIONS \\xa0\\xa0\\xa0\\nSEE PROFILE\\nDavid Pint o\\nBenemérit a Univ ersidad A utónoma de Puebla\\n184 PUBLICA TIONS \\xa0\\xa0\\xa01,620  CITATIONS \\xa0\\xa0\\xa0\\nSEE PROFILE\\nAll c ontent f ollo wing this p age was uplo aded b y Ashr af Uddin  on 18 F ebruar y 2017.\\nThe user has r equest ed enhanc ement of the do wnlo aded file.', metadata={'source': './CS-Research.pdf', 'page': 0}),\n",
       " Document(page_content='1 \\n \\nTITLE  PAGE  \\n \\nComputer Science Research: The Top 100 I nstitutions in India \\nand in the W orld \\n \\n \\nAuthors:  \\n \\nVivek Kumar Singh1, Ashraf Uddin1 and David Pinto2 \\n \\n1 Department of Computer Science,  \\nSouth Asian University, New Delhi, India  \\n \\n2 Faculty of Computer Science ,  \\nBenemérita Universidad Autonóma de Puebla, Puebla, Mexico  \\n \\n \\n \\n \\nCorresponding Author and Address  \\n \\nDr Vivek Kumar Singh  \\nDepartment of Computer Science  \\nSouth Asian University  \\nAkbar Bhawan, Chanakyapuri, New Delhi -110021  \\nEmail: vivekks12@gmail.com  \\nTel: +91 -11-24195148, +91 -9971995005  \\nWeb: http://www.sau.ac.in/~vivek/   \\nhttp:// www.viveksingh.in', metadata={'source': './CS-Research.pdf', 'page': 1}),\n",
       " Document(page_content='2 \\n \\nComputer Science Research: The Top 100 I nstitutions in India \\nand in the W orld \\n \\nVivek Kumar Singh1, Ashraf Uddin1 and David Pinto2 \\n \\n1 Department of Computer Science,  \\nSouth Asian University, New Delhi, India  \\nvivek@cs.sau.ac.in  and mdaakib18@gmail.com  \\n2 Faculty of Computer Science ,  \\nBenemérita Universidad Autonóma de Puebla, Puebla, Mexico  \\ndpinto@cs.buap.mx   \\n \\n \\nAbstract: This paper aims to perform a detailed scientometric and text -based analysis of Computer Science \\nresearch output of the 100 most productive institutions in India and in the world. The analytical characterization \\nis based on research output data indexed in Scopus durin g the last 25 year  period  (1989 -2013). Our \\ncomputational analysis involves a two -dimensional approach involving the standard scientometric methodology \\nand text -based analysis. The scientometric characterization aims to assess Indian research in CS domain v is-à-\\nvis the world top and to bring out the similarities and differences among them. It involves analysis along \\ntraditional scientometric in dicators such as total output, citation -based impact assessment, co -authorship  \\npatterns , international collaboration  levels  etc. The text -based characterization aims to identif y the key research \\nthemes and the ir temporal trends  for the two institution sets . The experimental work is one of its kinds  and \\nperhaps the only such work that identify  characteristic similarities and differences in CS research landscape of \\nIndian institutions vis -à-vis w orld institutions . The paper presents useful analytical results and relevant \\ninferences.   \\n \\nKeywords : Computer Science  Research , India, Information Technology, Informetrics, Scientometric s. \\nJEL Classification  I23 \\n \\n1. Introduction  \\nThe Information T echnology  (IT) plays a very vital role in progress of any nation  in the modern era . The \\nInformation and Communication T echnologies (ICT) , to a large extent , are  based on research and \\ndevelopment in Computer S cience (CS). The present century has witnessed rise of knowledge economies', metadata={'source': './CS-Research.pdf', 'page': 2}),\n",
       " Document(page_content='3 \\n \\nand nations across the world are investing more and more on the development of science and t echnology, \\nparticularl y ICT. Though, a late starter in CS domain research, India  has got more and focuses on it during \\nlast two decades  and has attained a respectable presence in the IT development sector . However, Indian \\ninstitutions are yet to attain the stage of being at for efront of knowledge creation in CS domain. In this \\ncontext, it is important to measure and assess  research competence of most productive Indian institutions \\nvis-à-vis the most productive world institutions . This paper present s our research work with this broader \\nmotivation  and presents  outcomes and inferences obtained using standard scientometric and a novel text -\\nbased analytical methodology. Our analysis involves a fairly large set of 100 most productive institutions in \\nIndia as well as in the world. The data for analysis is the CS domain research output for last 25 year period \\n(1989 -2013)  indexed in Scopus1.  \\n \\nWe have tried to do a systematic computational analysis  to characterize similarities and differences in the \\nresearch output of most productive inst itutions of India and the world. Our effort has been to present a \\ndetailed analytical characterization of CS research from most productive Indian institutions vis -à-vis the \\nmost productive institutions world wide. More precisely, we aim to answer following research questions:  \\n\\uf0b7 What are characteristic similarities and differences in CS domain research of most productive \\nIndian institutions and the most productive world institutions?   \\n\\uf0b7 What is India ’s contributi on to the total CS research worldwide and what is i ts relative impact?  \\n\\uf0b7 What major research themes are pursued by most productive Indian institutions and how does it \\nrelate to the research themes pursued in most productive world institutions?  \\n\\uf0b7 What inferences such a characterization may have for Indian inst itutions in CS domain research ?  \\nWe believe that t he results and discussion in the paper help in answering the research questions stated \\nabove  and few other related ones .   \\n \\nThe rest of the paper is organized as follows: Section 2 briefly describes some related  work . The s ection 3 \\npresents a brief overview of CS domain research in India. Section 4 describes the data collection  and the \\nsection 5 explains the methodology followed. The section 6 presents  results of scientometric indicator \\ncomputations for bo th Indian and world institutions. Section 7 present s analysis of Indian and world CS \\nresearch institution s using Exergy -based trajectory analysis. Section 8 describes the identification of \\n                                                           \\n1 http://www.scopus.com/', metadata={'source': './CS-Research.pdf', 'page': 3}),\n",
       " Document(page_content='4 \\n \\nresearch themes in the data and their trends. The section 9 present s a summary of the work and the major \\ninferences obtained . \\n \\n2. Related Work \\nThough, no previous work have done the kind of analytical characterization we attempted to do, we still list \\nsome past research  work focused on scientometric profiling for a country/ region and/ or a particular \\nsubject. Most of the se related research publications  are, however, limited to performing standard \\nscientometric analysis. Nevertheless, we list some of the research works that he lped us in identifying our \\nresearch questions and the comparative analysis methodology. A recent work (Fu, 2013)  studied the \\nindependent research of China and compare d the result s with the publication outputs of seven major \\nindustrialized countries. Anothe r work  (Kao, 2012 ) studied the improvement in management research in \\nTaiwan for a specific domain . Another related work  (Gupta , 2011) accessed several parameters such as total \\nresearch output, its growth, rank and global publication share, citation impact,  share of international \\ncollaborative papers of Indian CS research output during 1999 -2008.  One more recent study (Matthews , \\n2013)  tried to find typical ranges of two measures of individual productivity: number of papers and sum of \\nauthor share for physics  publications in South African universities. Another work  (De Souza , 2012)  aimed \\nto present the profile of the researchers, the pattern of scientific collaboration and the knowledge \\norganization in the area of information science in Brazil . In one of our r ecently published work  (Uddin , \\n2014 a), we tried to map the academic research  landscape of South Asian countries in volving  all disciplines. \\nAnother recent work (Uddin, 2014b ) did a similar analysis for the CS domain research in the SAARC \\nregion . Some r esearchers also dedicated their researches solely to explore the collaboration in different \\nscientific domains discovering collaboration patterns and its impact on research activities. One of such \\nworks (Tang , 2011)  analyzes the rapid growth of China in the field of nanotechnology and the rise of \\ncollaboration between China and the US in this emerging domain.  Similarly, some  other recent papers \\n(Ozel , 2012; Onyancha , 2011; Costa , 2013; Prathap , 2013; Teodorescu , 2011 ; Abramo , 2013; Viana , 2013; \\nOrtega , 2013; and Liu, 2013)  reported different aspects of collaboration , growth  and quantitative/ \\nqualitative measurements.  In most of these works, the techniques of presenting and analyzing research \\noutput vary over researchers’ need and similarly, the covered datasets are time and source dependent.  \\nThough, we learned lessons from the previous works listed here, we aimed to do a  different kind of \\nanalytical charact erization using both, the standard scientometric and a text -based computational analysis.', metadata={'source': './CS-Research.pdf', 'page': 4}),\n",
       " Document(page_content=\"5 \\n \\nTo the best of our knowledge, no previous work aimed to do the kind of analytical characterization, we \\nreport here for the CS domain research  in India (vis-a-vis the world ).  \\n \\n3. Overview of CS Research  in India   \\nCS as an academic discipline in the world began in a systematic manner in 1950s. The world's first \\ncomputer science degree program, the Cambridge Diploma in Computer Science, began at the University of \\nCambridge Computer Laboratory in 1953. The first computer science degree program in the United States \\nwas initiated at Purdue Universit y in 19622. The now well -known department of Computer Science in \\nStanford University was founded in 19653. Thereafter, a  large number of universities in developed countries \\nstarted CS education in the period of 19 60’s. In India, CS as an academic disciplin e was first introduced in \\n1963 by Indian Institute of Technology  (IIT)  Kanpur, with the start of “Computer -related” course s and the \\nfirst “CS classroom”, though an independent academic program in CS started in 19714. The IIT Madras CS \\ndepartment started in  19735, though initially as Computer center. Similarly, formal CS department started in \\nIIT Kharagpur in 19806 and IIT Bombay in 19827, in continuation to their initial efforts at teaching \\ncomputing courses. The department of Computer Science and Automatio n (CSA)8 at Indian Institute of \\nScience (I ISc) Bangalore started in 1969. It was originally called the “School of Automation” and continued \\nto be known with this name till the mid -eighties. In the university system, Jadavpur University started a \\npostgradua te diploma in CS in 1968, a first of its kind. A formal department of CS was carved out of \\ntrifurcation of the ECTE department in 19889. The  (Indian Statistical Institute)  ISI Calcutta started a two \\nyear M. Tech. program in CS in 1981, perhaps first such postgraduate program of its kind10. By the end of \\n1980s, many prominent institutions started academic programs/ departments in CS. However, in the early \\nperiod, the major focus was on education, training and manpower development. Research activity in CS \\nstarted to grow only in 1990s. The last two decades have seen CS research gaining momentum, with \\nestablishment of CS departments in all major universities/ institutions and establ ishment of specialized \\ninstitutions for Information Technology (IT) teaching and research. Some of these new specialized IT \\n                                                           \\n2 http://en.wikipedia.org/wiki/Computer_science  \\n3 http://cs.stanford.edu/  \\n4 http://www.cse.iitk.ac.in/dept/index.html  \\n5 http://www.cse.iitm.ac.in/  \\n6 http://cse.iitkgp.ac.in/  \\n7 http://www.cse.iitb.ac.in/page17  \\n8 http://www.csa.iisc.ernet.in/aboutus/aboutus.php  \\n9 http://www.jaduniv.edu.in/htdocs/view_department.php?deptid=59  \\n10 http://www.isical.ac.in/~mtechcs/\", metadata={'source': './CS-Research.pdf', 'page': 5}),\n",
       " Document(page_content='6 \\n \\ninstitutions have gained a very respectable presence in CS research in a very short span of time. Some of the \\nmajor IT companies hav e also started their research laboratories in India during the last 2 -3 decades, such as \\nIBM Research in 1988 and Microsoft Research in 2005. Some applied research in CS domain is now also \\ncarried out at various government laboratories/ organizations, such  as CSIR11, DRDO12, ISRO13 etc. \\n \\nIn order to have a more informed picture of the current status of CS research in India, we have collected \\ndata for  CS domain research output of all institutions in India , for the 25 year period from 1989 to 2013 , \\nfrom the Scop us database . This period happens to be the period of actual growth in CS research in India. \\nWe plot  the year-wise proportionate contribution of India  to the total CS domain research worldwide in \\nFigure 1 . We see that India’s share to the total CS research output of the world is increasing in general, \\nexcept in few years . In terms of percentage contribution, it increased from 1.88% in 1989 to 4.95% in 2013 . \\nThe rate of growth is more clearly visible from 2000-01. Also, computing the rates of growth of CS \\nresearch in India and the total world tells us that India ’s average rate of growth is higher than the world . \\n \\n4. Data Collection  \\nWe have collected data of CS research output for a period of 25 years (1989 -2013)  from Scopus, a well -\\nknown bibliometric database . A total of 2 ,876,512 publication records are (were)  found for the period 1989 -\\n2013  for the whole world , where India contributes 84,385 ( about 2.93%) research papers , as on March \\n2014 . We collected data ins titution -wise for 100 most productive institutions from  India (referred to as \\nI100) and the world  (referred to as W 100). Our collection, though  not exhaustive , is a good representative \\nof research output data . The collected representative data covers 846,527 846,526 records ( about 29.43%  \\n29.6%) of the world CS research output  and 59,682 59,619  records ( 70.73%  71%) of the total Indian CS  \\nresearch output . Out of a total of 61,502 records in Scopus for  Indian CS  research output  (I100) , 1,314 \\nentries  have missing year value and about 506 duplicate entries , which leave  unique records as 59,682. Out \\nof a total 60,119 records in Scopus for Indian CS research output (I100), duplicate records were removed, \\nwhich leave 59 ,619 unique records. Similarly, for world universities  in CS research (W100), 846,527  \\n846,526  are unique records as against 924,575 records originally found in data .  In addition to CS domain \\nresearch output of I100 and W100, we also collected some related statistics for India and world, such as \\n                                                           \\n11 http://www.csir.res.in/  \\n12 www.drdo.gov.in  \\n13 http://www.isro.org/', metadata={'source': './CS-Research.pdf', 'page': 6}),\n",
       " Document(page_content='7 \\n \\nGross Expenditure on Research and Development (GERD), Researchers per million people etc. The table 1  \\nshows these values to establish a broader background perspective for our analytical characterization.  \\n \\n5. Methodology  \\nOur analytical characterization methodology is primarily a two -dimensional ap proach, comprising of a \\nstandard scientometric analysis and a computational text analysis. First, w e perform a scientometric \\ncharacterization along the six main indicators, namely Total Publication (TP), Total C itations (TC), \\nAverage Citation Per Paper (ACPP), HiCP (Highly Cited Papers), Internationally Collaborated P apers (ICP) \\nand the cited percentage . Out of these, o nly TP i s a primary indicator, and all other secondary indicators are \\ncomputed programmatically  from the data.  We have computed these indicators for all the institutions in \\nboth the sets. Second major part of our analysis is to characterize the major research themes pursued and \\nadvanced by the institutions in the set. For this task, we have processe d the entire data to classify every \\nresearch output in one (or more) of the 11 major thematic areas in CS research, as illustrated in Appendix. \\nThis thematic mapping is more informative than the subcategory labels used by the Scopus database  and is \\nextende d from a recent previous work (Uddin & Singh, 2015) . For classifying a paper to belong to a \\nthematic area, its ‘author keyword’, ‘paper title’ and ‘abstract’ fields are analyzed. A paper is assigned a \\nthematic class which has substantial match of keywords with the predefined thematic classes. After \\nattributing thematic areas to each paper, we measured the thematic area strength of all the institutions and \\nidentified leading institutions in a particular thematic area  (Thematic trend, we have done for I100 &  W100 \\nas a whole but not institution -wise). This has been done for the two sets, I100 and W100. While, the \\nscientometric analysis helps us in analytical and comparative characterization of research competitiveness \\nof I100 and W100 sets, t he text -based comp utational analysis helps us in identifying leading research \\nthemes pursued by institutions in the two sets and the similarities and differences  among them .   \\n \\n6. Scientometric Indicator Computations  \\nWe would first present the computational results of analytic al characterization using scientometric \\nindicators. The results compare and characterize the two sets, I100 and W100, along different indicators as \\nsummarized in the four subsections below.   \\n \\n6.1 Research Output', metadata={'source': './CS-Research.pdf', 'page': 7}),\n",
       " Document(page_content='8 \\n \\nWe first tried to do a detailed indicator -based comparison of institutions in I 100 and W100. Before, we \\npresent the results;  it would be relevant to mention here that no Indian institution figures in the W100 set of \\n100 most productive institutions worldwide . The relative rank on product ivity of Indian institutions can be \\nunderstood from the fact that total 25 year period CS research output of 100th institution (University College \\nLondon) in W100 is 5,747 whereas , the output of the two top ranking Indian institutions, namely Indian \\nInstitute of Technology, Kharagpur and Indian Institute of Science, Bangalore are 3,514 and 3,506, \\nrespectively. The most productive Indian institution has output levels below 150th rank institution in W100. \\nIt may also be worth mention ing that in a  recently published ranking by ARWU14, only two Indian \\ninstitutions, “Indian Statistical Institute” and “Indian Institute of Science”, are placed in the top 200 list, \\nunder  CS subject category.  We present the detailed statistics of scientometric indicator -based comparative \\nassessment in tables 2 and 3. The table 2 present s comparative figures for the sets, I100 and W100, as a \\nwhole. Form the tables; w e observe that there is a significant difference among indicators like ACPP (I100 \\naverage being 2.66 and W100 average 9.37), HiCP (average value of 60 for I100 and 855 for W100), ICP \\n(110 for I100 as against 2419 for W100) and cited percentage (39.33% for I100 and 58.91% for W100). \\nThe table 3 presents values of various scien tometric indicators for 20 most productive institutions in I100 \\nand W100, for a more detailed comparison.    \\n \\n6.2 Co -authorship Pattern s \\nThe second important aspect we addressed is about the c o-authorship and collaboration patterns seen in the \\nCS research in top Indian institutions and the top institutions worldwide. The authorship pattern for research \\npapers has known to have effect on their overall impact. A paper with multiple (more than one) authors is \\nthe joint contribution of two or more authors  and has inputs from different people/ groups. Higher multi -\\nauthored papers are an indicator for higher interdisciplinary research and/ or higher level of collaboration \\nbetween researchers at national or interna tional level. Research projects involving bigger groups also result \\nin higher multi -authored papers. From the data, we observed the co-authorship  pattern s for India, and found \\nthat about 39.4% of the total CS research outputs of I100 involve two authors and about 32% with three \\nauthors. Only  about 5% of the output is by a single author , indicating that about 95% of the total CS output \\ninvolves some form of collaboration. The co -authorship pattern for the W100 , however, shows different \\nscenario  where the co -authorship distribution is less skewed (0.91) than that of Indian CS co -authorship \\n                                                           \\n14 http://www.shanghairanking.com/SubjectCS2014.html', metadata={'source': './CS-Research.pdf', 'page': 8}),\n",
       " Document(page_content='9 \\n \\ndistribution (1.51). The Figure s 2(a) and 2(b) show year -wise trends of co -authorship patterns f or the \\ncollected data. There is a clear trend towards higher co llaboration between researchers, as measured by \\nmultiple authors in more and more research papers. In the most recent year, about 50% of the total CS \\nresearch for W100  is with three or more authors,  while it is about 30% for the I100.   \\n \\n6.3 International Collaboration Patterns  \\nAfter observing a higher proportion of multi -authored papers, we aimed to measure the types of \\ncollaboration patterns as national or international. Higher international collaboration is not only an indicator \\nof multi -national collabo ration but also shows maturity and international nature of a discipline. A study  \\n(Smith 1958)  was perhaps the first work on collaboration and it propose d that collaborative papers can be \\nused as a proxy measure of collaboration  among researchers.  In another work, Glanzel  (2001)  show ed that \\ninternational co -authorship, on average, results  in publications with higher citation rates than purely \\ndomestic papers. Through computational analysis, we found  that 95.5%  of the I100 are co-authored  (with \\naverage co -authorship value as 2.92) , whereas 93.5%  of the W100  are co-authored  (but average co -\\nauthorship value as 3.41) . The average co -authorship for W100  is higher, an indicator of higher level of \\ncollaboration . The Figure 3  shows collaboration patterns for outputs of I100 and W100 . We observe that \\nI100 has 25% of the collaborated papers involving international collaboration as against 30% value for \\nW100 . We also identified the top collaborating countries by Indian authors. The Table 2 shows the list of \\ntop 20 countries  having highest collaboration in I100 output during the 25 year period. The United States  of \\nAmerica  contributes largest share of I100’s international collaboration ( with 4316  research papers) followed \\nby Singapore  (849 research papers) and  Canada ( 801 research papers ). The W100  data shows higher \\ninternational collaborative research papers, including multi -country collaboration outputs.  \\n \\n6.4 Citation -based Impact  Assessment  \\nAn independent report by Thompson Reuters showed that India had about 0.52% of its national output (all \\ndisciplines) in the worldwide HiCP list in 2011  (Thompson Reuters Report on The Research and Innovation \\nPerformance of G20, March 2014 ). It is well acce pted that s olely counting number of research publications \\nmay not be the best method  to measure the scientific research productivity  and usefulness . It focuses \\nprim arily on the output quantity and not on the quality as measured through impact/ productivity . It would \\nnaturally be unfair to treat two research papers, one with very high citation count and the other with low or', metadata={'source': './CS-Research.pdf', 'page': 9}),\n",
       " Document(page_content='10 \\n \\nno citation, as equally useful. On the other hand, a research paper with higher citation is considered more \\neffective, useful and productive.    Citation count is considered an effective measure of productivity and \\ninfluence of a research paper. We have extracted total citations of each research paper in the I100 and \\nW100  data and also identif ied the top cited papers  (top 10% most cit ed papers)  in each group for each of the \\n25 years. We track the institution information for all such records and look for top contributing institutions \\nto the set of highly cited (top 10%) publications. The figure 4(a) shows the number of collaborated pape rs \\namong the top 10% highly cited papers  in I100. We find that t otal of 1797 papers (app roximately  30% \\namong the top 10% cited papers ) are collaborated internationally.  After an assessment of relationship \\nbetween collaboration and citation -impact, we moved to identify the top institutions in I100 with highest \\nproportions of top 10% highly cited papers  in I100. The  figure 4(b)  shows top five institutions for each year \\nchronologically. The vertical axis represents the contribution in percentage to the top10% cited papers in \\nthe particular year. The figure clearly shows that initially only certain selected institutions contributed to th e \\nelite set of papers and grabbed maximum of the set. However, o ver the period of time other institutions also \\ncontributed to the top 10% highly cited papers, an indicator of quality research output being generated from \\nmore and more institutions. On an av erage, 12 institutions from India are the most frequent  contribut ors to \\nthe top cited papers in I100 set. The Indian Institute of Science (IIS) showed almost omnipresence  in all the \\nyears , except 1989 , indicating most consistent qualitative output . The Indian Institute of Technology, \\nKharagpur (IITK) , Indian Institute of Technology, Delhi (IITD) and Indian Statistical Institute, Kolkata \\n(ISIK) are few other frequent names in the list. We also measured the proportion of CS research output in \\nI100 and W100  that remains un -cited. The figure 4(c) shows the percentage of cited and un -cited papers  for \\nboth I100 and W100 , shown  over five consecutive chronological blocks  of five years each. The problematic \\nindication observed is that the percentage of papers that are cited is decreasing over the time. This may be \\ndue to smaller citation window availability or possibly with higher amount research output, a large part of \\nwhich is not that useful/ productive. On a comparison between I100 and W100  data, we observe that the \\ncorresponding cited figures (in percentage) for first two blocks are almost same but the values for W100  in \\nrecent three blocks are higher, implying low  immediacy index  for CS research output from India . \\n \\n \\n7. Performance based on  Trajectory A nalysis  \\nA recent paper (Nishy 2012) proposed a n exergy rank based on a thermodynamic analogy . The exergy \\nindicator (referred as X) is a multiplicative product of quality and quantity of a scientist’s or research', metadata={'source': './CS-Research.pdf', 'page': 10}),\n",
       " Document(page_content='11 \\n \\ngroup’s performance . The measure is derived from primary/ secondary indicators like impact, citations and \\nnumber of papers . The proposed system tries to define a single meaningful indicator that combines both, the \\nquantit y (number of papers published , P) and the quality (measured as impact , i=C/P, where C is citation \\ncount ) aspects. A quality proxy, called exergy is defined as X=(C/P)*C . The values of i and P or i and C \\ncan be shown as  trajector ies on two -dimensional plot with respect to  time (known as iPX and iCX \\napproaches, respectively) . Both the iCX (the contour lines for X=iC ) and iPX (the contour lines for X=i2P) \\ncan be chosen for two dimensional mapping of the information. The iPX approach is more intuitive as it \\nshows the direct relation between quantity ( P) and quality ( i) which is of our interest but in iCX approach \\noutput P is hidden and does not directly reflect the quantity ( P). We have used the exergy measure to \\ncompute quantity -quality composite rank for some selected institutions in I100 and W 100.  \\n \\nWe have measured th e research performance of the top 5 Indian institutions in I100 and plotted them with \\nthe top 5  institutions in W100  list based on their TP value . This is done  primarily to compare the relative \\nperformance of Indian institutions vis -à-vis the institutions in world top 100. Unfortunately, no Indian \\ninstitution appears in the W100  list. The figure 8(a) displays  the exergy values for the selected institutions . \\nHere, the X-axis represents the time in year and Y-axis denotes the obtained exerg y score in that particular \\nyear. Similarly, two sets of institutions were selected based on ACPP and on HiCP values. Selecting \\ninstitutions on the values of different indicators help us to see the performance variability of institutions  \\nleading on different indicators’ scale. Figures 8(b) and 8(c)  shows the performance in exergy for institutions \\nselected based on ACPP and HiCP, respectively.  All the plots are made f or eight 5 year rolling windows , \\nfrom 2001 -2005 to 2008 -2012  to see the pe rformance in the following 6th year (2006 -2013) . We see that all \\nselected Indian institutions ’ performances  are below all selected W100 institutions’ performances , a result \\nthat needs serious thought. Among the top Indian institutions  in I100, most of them  are IITs . All the exergy \\ngraphs show  that in terms of performance (quantity -quality) measure Indian institutions are far behind than \\nthat of W100  institutions. The important thing here to mention is that we did not consider the academic \\ninfrastructure suc h as number of faculties, active researchers etc in the context . The data and computed \\nvalues are shown in supplementary table 5(a-c). The supplementary table s show the paper published ( P) by \\nthe leading institutions over a five year publication window and  citations ( C) obtained in the next year for \\neach publication window. These values are used to compute the impact ( i=C/P ) and the exergy ( X=C2/P).', metadata={'source': './CS-Research.pdf', 'page': 11}),\n",
       " Document(page_content='12 \\n \\n8. Identifying Research T hemes in Output  \\nIn addition to the scientometric analysis, we also performed a text -based analysis of the data to identify \\nmajor research topics/ themes and their variations with respect to time. We used the burst detection \\nalgorithm (Kleinberg 2003)  for this task . We app lied the algorithm on the “indexed keyword ” field of our \\ndata set  for the entire 25 year period. The burst detection measures the keyword weights which is a function \\nof the occurrence  frequency and the time span of the keyword.  The Science of Science (Sci2 )15 was used for \\ntemporal analysis (Burst Detection).   A keyword with a heavy weight indicates that the topic was \\nresearched frequently for a significant period of time. The figure s 6(a) and 6(b) show keyword density \\nplots, based on their corresponding weight s, for I100 and W100  data, respectively. For the visualization \\npart, we used VOSviewer16 reflecting the weights of corresponding keyword s. We can see that “Computer \\nSimulation” and “Mathematical Models” are present in leading positions in both figures. Similarly \\n“Problem Solving” appear s in both plots . However, the W100  plot shows a significan ce of topics like  \\n“Algorithms”, “Approximation Theor y”, “Network Protocols” and “ Computational Complexity” ; where the \\nI100 plots show significance of topics like “Computer Networks”, “Information Technology”, \\n“Communication” and “Computational Methods” , for the entire 25 year period considered. In \\nsupplemen tary figures 6 (c ) to 6(g), we show the tag cloud plot visualization of the major research themes \\nfor the I -Top10  in the left and W100  in the right .   The tag -cloud for the su b-period 1989 -1993 shows that \\n“Algorithm”, “M athematical Models”, “Computer S imulation” were  the leading topics in I100, whereas \\nW100  had frequent occurrence of “Algorithm”, “Computer S imulation ”, “Optimization” and “Image \\nProcessing” . In the 1994 -1998  block , “Algorithms”, “Communication”, “O ptimizat ion”, “Information \\nTechnology” &  “Artificial I ntelligence” were the top topics  in I100, whereas  “Mathematical M odels” & \\n“Computer S imulation” dominated in W100 . In 1999 -2003, “Mathematical M odels” again took the leading \\nposition  followed by “A lgorithms  for I100 and “Mathematical Models ” & “Optimization ” dominated in \\nW100 . In 2004 -2008, “Mathematical M odels” became more  significant as compared to “Algorithm”  for \\nI100 and in W100 , “Problem S olving” & “Optimization” were prominent . In 2009 -2013, we can see a slight \\nchange from previous block’s trend s, as  “Computer Simulation” & “A lgorithms” appeared as major \\nresearch themes for I100, whereas W100  shows a variation including “O ptimization”, “ Artificial \\nIntelligence” & “Signal P rocessing” . In general, “Algorithm” , “Computer S imulation”  & “Mathematical \\n                                                           \\n15 https://sci2.cns.iu.edu/user/index.php  \\n16 http://www.vosviewer.com/', metadata={'source': './CS-Research.pdf', 'page': 12}),\n",
       " Document(page_content='13 \\n \\nModels”  are prominent research themes i n both I100 & W100  data. The W100  data also shows  \\n“optimization” as a frequent research t heme .   \\n \\n9. Summary and Conclusion  \\nThe paper  reports our results on scientometric, network -theoretic and text -based analysis of CS research in \\nmajor institutions in India ( I100 as representative sample) and in the world ( W100  as representative sample) \\nduring the last 25 year period (1989 -2013). The analytical results present  measurements the characteristic \\nsimilarities and differences between I100 and W100 showing primary and secondary indicators of CS \\nresearch , such as absolute and comparative growth , collaboration patterns, year-wise citation -based impact \\nassessment  etc. We have also att empted to show the contribution of Indian institutions to world CS output \\nand the impact of Indian CS output . The Quality -Quantity Exergy performance measures were shown and \\ndiscussed for top institutions from I100 and W100 . Finally , we report outcome of r esearch theme trend \\nmapping through a text -based analysis  of the data. A year -block wise temporal trend  identification of \\nresearch theme is done along with burst detection of CS research output data and visualized graphically.  \\n \\nThe results found indicate t hat India has a lot to enhance it research activities with quality. Although India \\nmaintained the same trends in CS research but the quality research is still lacking. The results report \\nanalysis of a substantial period of 25 years and can be used in many ways :  to take a look at CS research in \\nIndia vis -à-vis top ranking world institutions;  to formulate polic ies for promoting research and enhancing \\nquality output ; to help a student selecting an institution for his/her research ; and other academic/ scientific \\npurposes.   \\n \\nAcknowledgement  \\nThis work is supported by research grants from Department of Science and Technology, Government of India \\n(Grant: INT/MEXICO/P -13/2012) and University Grants Commission of India (Grant: F. No. 41 -624/ \\n2012(SR)).  \\n \\nReferences  \\nAbramo, G., D’Angelo, C.A.  & Murgia, G. (2013). The collaboration behaviors of scientists in Italy: A \\nfield level an alysis. Journal of Informetrics , 7(2), 442 -454.  \\nCosta, B.M.G., Pedro, E.S.  & Macedo, G.R. (2013). Scientific c ollaboration in biotechnology: the case of \\nthe northeast region in Brazil. Scientometrics, 95(2), 571 –592.', metadata={'source': './CS-Research.pdf', 'page': 13}),\n",
       " Document(page_content='14 \\n \\nDe Souza, C.G. & Ferreira, M.L. A. (2012).  Researchers profile, co -authorship pattern and knowledge \\norganization in information science in Brazil. Scientometrics, 95(2), 673 –687. \\nFu, H.Z, & Ho, Y.S. (2013). Independent research of China in Science Citation Index Expanded during \\n1980 –2011. Journal of Informetrics, 7(1), 210 -222. \\nGlänzel, W. (2001). National characteristics in international scientific co -authorship relations. \\nScientometrics, 51(1), 69 –115. \\nGupta, B.M., Kshitij, A.  & Verma, C. (2011). Mapping of Indian computer science research output , 1999 –\\n2008. Scientometrics, 86(2), 261 –283. \\nKao, C., Liu, S. &  Pao, H. (2012). Assessing improvement in management research in Taiwan. \\nScientometrics, 92(1), 75 –87. \\nKleinberg, J. (2003). Bursty and Hi erarchical Structure in Streams.  Data Mining and Knowle dge Discovery, \\n7, 373 –397. \\nLiu, Y., Rousseau, R.  & Guns., R. (2013). A layered framework to study collaboration as a form of \\nknowledge sharing and diffusion. Journal of Informetrics, 7(3), 651 -664. \\nMa, R.., Ni, C.  & Q iu, J. (2008). Scientific research competitiveness of world universities in computer \\nscience. Scientometrics, 76(2), 245 –260. \\nMatthews, A.P. (2013). Physics publication productivity in South African universities. Scientometrics. \\n95(1), 69 –86. \\nNishy, P., Panwa r, Y., Prasad, S., Mandal, G.K.  & Prathap, G. (2012). An impact -citations -exergy (iCX) \\ntrajectory analysis of leading research institutions in India. Scientometrics, 91(1), 245 -251. \\nOnyancha O.B.  & Maluleka, J.R. (2011). Knowledge production through collaborative research in sub -\\nSaharan Africa: how much do countries contribute to each other’s knowledge output and citation impact?. \\nScientometrics, 87(2), 315 –336. \\nOrtega, J.L.  & Aguillo, I.F. (2013). Institutional and country collaboration in an online service of scientific \\nprofiles: Google  Scholar Citations. Journal of Informetrics, 7(2), 394 -403. \\nOzel, B. (2012). Collaboration structure and knowledge diffusion in Turkish management academia. \\nScientometrics, 93(1), 183 –206. \\nPrathap, G. (2013). Second order indicators for evaluating internat ional scientific collaboration. \\nScientometrics, 95(2), 563 –570.  \\nSmith, M. (1958). The trend toward s multiple authorship in psychology. American Psychologist, 13(10), \\n596–599. \\nTang, L.  & Shapira, P. (2011). China –US scientific collaboration in nanotechnolo gy: patterns and \\ndynamics. Scientometrics, 88(1), 1 –16. \\nTeodorescu, D.  & Andrei, T. (2011). The growth of international collaboration in East European scholarly \\ncommunities: a bibliometric analysis of journal articles published between 1989 and 2009. Scien tometrics, \\n89(2),  711–722. \\nUddin, A.  & Singh, V.K. (2014). Mapping the c ompute r science research in SAARC c ountries. IETE \\nTechnical Review, 31(4), 287-296.  \\nUddin, A.  & Singh, V.K. (2014).  Measuring research output and collaboration in South Asian countri es. \\nCurrent Science , 107(1), 31 -38. \\nUddin, A. & Singh, V.K. (2015). A Quantity -Quality Composite Ranking of Indian Institutions in \\nComputer Science Research. IETE Technical Review  (forthcoming) DOI: \\nhttp://dx.doi.org/10.1080/02564602.2015.1010614', metadata={'source': './CS-Research.pdf', 'page': 14}),\n",
       " Document(page_content='15 \\n \\nViana , M.P., Amancio, D .R. & Costa, L.F. (2013). On time -varying collaboration networks. Journal of \\nInformetrics, 7(2), 371 -378. \\nView publication stats', metadata={'source': './CS-Research.pdf', 'page': 15})]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_splitter = CharacterTextSplitter(chunk_size=8000, chunk_overlap=1000)\n",
    "texts = text_splitter.split_documents(result)\n",
    "texts[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "ded10663",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = OpenAIEmbeddings(openai_api_key='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')\n",
    "docsearch = Chroma.from_documents(texts, embeddings)\n",
    "from langchain.chains import RetrievalQA\n",
    "qa = VectorDBQA.from_chain_type(llm=OpenAI(), chain_type=\"map_reduce\", vectorstore=docsearch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "2473836f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the Question you want to ask from your model: what is the summary of this video\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' This video is about loading YouTube videos with transcripts and using LangChain to generate a summary of the video. It explains how to split up a long video into documents that OpenAI can understand, as well as how to use MapReduce to get a summary of multiple videos at once.'"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = input(\"Enter the Question you want to ask from your model: \")\n",
    "qa.run(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e618f659",
   "metadata": {},
   "source": [
    "# PDFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cc9d36f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "\n",
    "def file_loader(path):\n",
    "    return PyPDFLoader(path)\n",
    "\n",
    "# loader = file_loader(\"./LangChain-Documentation.txt\") \n",
    "loader = file_loader(input(\"Enter the path of your text file:\"))\n",
    "\n",
    "documents = loader.load()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d809d635",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='See discussions, st ats, and author pr ofiles f or this public ation at : https://www .researchgate.ne t/public ation/277941134\\nComputer science research: the top 100 institutions in India and in the world\\nArticle \\xa0\\xa0 in\\xa0\\xa0Scient ome trics  · August 2015\\nDOI: 10.1007/s11192-015-1612-8\\nCITATIONS\\n44READS\\n10,015\\n3 author s:\\nSome o f the author s of this public ation ar e also w orking on these r elat ed pr ojects:\\nScient o-text View pr oject\\nDocument L evel Sentiment Analysis  View pr oject\\nVivek K umar Singh\\nBanar as Hindu Univ ersity\\n202 PUBLICA TIONS \\xa0\\xa0\\xa02,459  CITATIONS \\xa0\\xa0\\xa0\\nSEE PROFILE\\nAshr af Uddin\\nSouth Asian Univ ersity\\n34 PUBLICA TIONS \\xa0\\xa0\\xa0732 CITATIONS \\xa0\\xa0\\xa0\\nSEE PROFILE\\nDavid Pint o\\nBenemérit a Univ ersidad A utónoma de Puebla\\n184 PUBLICA TIONS \\xa0\\xa0\\xa01,620  CITATIONS \\xa0\\xa0\\xa0\\nSEE PROFILE\\nAll c ontent f ollo wing this p age was uplo aded b y Ashr af Uddin  on 18 F ebruar y 2017.\\nThe user has r equest ed enhanc ement of the do wnlo aded file.', metadata={'source': './CS-Research.pdf', 'page': 0}),\n",
       " Document(page_content='1 \\n \\nTITLE  PAGE  \\n \\nComputer Science Research: The Top 100 I nstitutions in India \\nand in the W orld \\n \\n \\nAuthors:  \\n \\nVivek Kumar Singh1, Ashraf Uddin1 and David Pinto2 \\n \\n1 Department of Computer Science,  \\nSouth Asian University, New Delhi, India  \\n \\n2 Faculty of Computer Science ,  \\nBenemérita Universidad Autonóma de Puebla, Puebla, Mexico  \\n \\n \\n \\n \\nCorresponding Author and Address  \\n \\nDr Vivek Kumar Singh  \\nDepartment of Computer Science  \\nSouth Asian University  \\nAkbar Bhawan, Chanakyapuri, New Delhi -110021  \\nEmail: vivekks12@gmail.com  \\nTel: +91 -11-24195148, +91 -9971995005  \\nWeb: http://www.sau.ac.in/~vivek/   \\nhttp:// www.viveksingh.in   \\n \\n \\n \\n \\n \\n \\n \\n ', metadata={'source': './CS-Research.pdf', 'page': 1}),\n",
       " Document(page_content='2 \\n \\nComputer Science Research: The Top 100 I nstitutions in India \\nand in the W orld \\n \\nVivek Kumar Singh1, Ashraf Uddin1 and David Pinto2 \\n \\n1 Department of Computer Science,  \\nSouth Asian University, New Delhi, India  \\nvivek@cs.sau.ac.in  and mdaakib18@gmail.com  \\n2 Faculty of Computer Science ,  \\nBenemérita Universidad Autonóma de Puebla, Puebla, Mexico  \\ndpinto@cs.buap.mx   \\n \\n \\nAbstract: This paper aims to perform a detailed scientometric and text -based analysis of Computer Science \\nresearch output of the 100 most productive institutions in India and in the world. The analytical characterization \\nis based on research output data indexed in Scopus durin g the last 25 year  period  (1989 -2013). Our \\ncomputational analysis involves a two -dimensional approach involving the standard scientometric methodology \\nand text -based analysis. The scientometric characterization aims to assess Indian research in CS domain v is-à-\\nvis the world top and to bring out the similarities and differences among them. It involves analysis along \\ntraditional scientometric in dicators such as total output, citation -based impact assessment, co -authorship  \\npatterns , international collaboration  levels  etc. The text -based characterization aims to identif y the key research \\nthemes and the ir temporal trends  for the two institution sets . The experimental work is one of its kinds  and \\nperhaps the only such work that identify  characteristic similarities and differences in CS research landscape of \\nIndian institutions vis -à-vis w orld institutions . The paper presents useful analytical results and relevant \\ninferences.   \\n \\nKeywords : Computer Science  Research , India, Information Technology, Informetrics, Scientometric s. \\nJEL Classification  I23 \\n \\n1. Introduction  \\nThe Information T echnology  (IT) plays a very vital role in progress of any nation  in the modern era . The \\nInformation and Communication T echnologies (ICT) , to a large extent , are  based on research and \\ndevelopment in Computer S cience (CS). The present century has witnessed rise of knowledge economies ', metadata={'source': './CS-Research.pdf', 'page': 2}),\n",
       " Document(page_content='3 \\n \\nand nations across the world are investing more and more on the development of science and t echnology, \\nparticularl y ICT. Though, a late starter in CS domain research, India  has got more and focuses on it during \\nlast two decades  and has attained a respectable presence in the IT development sector . However, Indian \\ninstitutions are yet to attain the stage of being at for efront of knowledge creation in CS domain. In this \\ncontext, it is important to measure and assess  research competence of most productive Indian institutions \\nvis-à-vis the most productive world institutions . This paper present s our research work with this broader \\nmotivation  and presents  outcomes and inferences obtained using standard scientometric and a novel text -\\nbased analytical methodology. Our analysis involves a fairly large set of 100 most productive institutions in \\nIndia as well as in the world. The data for analysis is the CS domain research output for last 25 year period \\n(1989 -2013)  indexed in Scopus1.  \\n \\nWe have tried to do a systematic computational analysis  to characterize similarities and differences in the \\nresearch output of most productive inst itutions of India and the world. Our effort has been to present a \\ndetailed analytical characterization of CS research from most productive Indian institutions vis -à-vis the \\nmost productive institutions world wide. More precisely, we aim to answer following research questions:  \\n\\uf0b7 What are characteristic similarities and differences in CS domain research of most productive \\nIndian institutions and the most productive world institutions?   \\n\\uf0b7 What is India ’s contributi on to the total CS research worldwide and what is i ts relative impact?  \\n\\uf0b7 What major research themes are pursued by most productive Indian institutions and how does it \\nrelate to the research themes pursued in most productive world institutions?  \\n\\uf0b7 What inferences such a characterization may have for Indian inst itutions in CS domain research ?  \\nWe believe that t he results and discussion in the paper help in answering the research questions stated \\nabove  and few other related ones .   \\n \\nThe rest of the paper is organized as follows: Section 2 briefly describes some related  work . The s ection 3 \\npresents a brief overview of CS domain research in India. Section 4 describes the data collection  and the \\nsection 5 explains the methodology followed. The section 6 presents  results of scientometric indicator \\ncomputations for bo th Indian and world institutions. Section 7 present s analysis of Indian and world CS \\nresearch institution s using Exergy -based trajectory analysis. Section 8 describes the identification of \\n                                                           \\n1 http://www.scopus.com/  ', metadata={'source': './CS-Research.pdf', 'page': 3}),\n",
       " Document(page_content='4 \\n \\nresearch themes in the data and their trends. The section 9 present s a summary of the work and the major \\ninferences obtained . \\n \\n2. Related Work \\nThough, no previous work have done the kind of analytical characterization we attempted to do, we still list \\nsome past research  work focused on scientometric profiling for a country/ region and/ or a particular \\nsubject. Most of the se related research publications  are, however, limited to performing standard \\nscientometric analysis. Nevertheless, we list some of the research works that he lped us in identifying our \\nresearch questions and the comparative analysis methodology. A recent work (Fu, 2013)  studied the \\nindependent research of China and compare d the result s with the publication outputs of seven major \\nindustrialized countries. Anothe r work  (Kao, 2012 ) studied the improvement in management research in \\nTaiwan for a specific domain . Another related work  (Gupta , 2011) accessed several parameters such as total \\nresearch output, its growth, rank and global publication share, citation impact,  share of international \\ncollaborative papers of Indian CS research output during 1999 -2008.  One more recent study (Matthews , \\n2013)  tried to find typical ranges of two measures of individual productivity: number of papers and sum of \\nauthor share for physics  publications in South African universities. Another work  (De Souza , 2012)  aimed \\nto present the profile of the researchers, the pattern of scientific collaboration and the knowledge \\norganization in the area of information science in Brazil . In one of our r ecently published work  (Uddin , \\n2014 a), we tried to map the academic research  landscape of South Asian countries in volving  all disciplines. \\nAnother recent work (Uddin, 2014b ) did a similar analysis for the CS domain research in the SAARC \\nregion . Some r esearchers also dedicated their researches solely to explore the collaboration in different \\nscientific domains discovering collaboration patterns and its impact on research activities. One of such \\nworks (Tang , 2011)  analyzes the rapid growth of China in the field of nanotechnology and the rise of \\ncollaboration between China and the US in this emerging domain.  Similarly, some  other recent papers \\n(Ozel , 2012; Onyancha , 2011; Costa , 2013; Prathap , 2013; Teodorescu , 2011 ; Abramo , 2013; Viana , 2013; \\nOrtega , 2013; and Liu, 2013)  reported different aspects of collaboration , growth  and quantitative/ \\nqualitative measurements.  In most of these works, the techniques of presenting and analyzing research \\noutput vary over researchers’ need and similarly, the covered datasets are time and source dependent.  \\nThough, we learned lessons from the previous works listed here, we aimed to do a  different kind of \\nanalytical charact erization using both, the standard scientometric and a text -based computational analysis. ', metadata={'source': './CS-Research.pdf', 'page': 4}),\n",
       " Document(page_content=\"5 \\n \\nTo the best of our knowledge, no previous work aimed to do the kind of analytical characterization, we \\nreport here for the CS domain research  in India (vis-a-vis the world ).  \\n \\n3. Overview of CS Research  in India   \\nCS as an academic discipline in the world began in a systematic manner in 1950s. The world's first \\ncomputer science degree program, the Cambridge Diploma in Computer Science, began at the University of \\nCambridge Computer Laboratory in 1953. The first computer science degree program in the United States \\nwas initiated at Purdue Universit y in 19622. The now well -known department of Computer Science in \\nStanford University was founded in 19653. Thereafter, a  large number of universities in developed countries \\nstarted CS education in the period of 19 60’s. In India, CS as an academic disciplin e was first introduced in \\n1963 by Indian Institute of Technology  (IIT)  Kanpur, with the start of “Computer -related” course s and the \\nfirst “CS classroom”, though an independent academic program in CS started in 19714. The IIT Madras CS \\ndepartment started in  19735, though initially as Computer center. Similarly, formal CS department started in \\nIIT Kharagpur in 19806 and IIT Bombay in 19827, in continuation to their initial efforts at teaching \\ncomputing courses. The department of Computer Science and Automatio n (CSA)8 at Indian Institute of \\nScience (I ISc) Bangalore started in 1969. It was originally called the “School of Automation” and continued \\nto be known with this name till the mid -eighties. In the university system, Jadavpur University started a \\npostgradua te diploma in CS in 1968, a first of its kind. A formal department of CS was carved out of \\ntrifurcation of the ECTE department in 19889. The  (Indian Statistical Institute)  ISI Calcutta started a two \\nyear M. Tech. program in CS in 1981, perhaps first such postgraduate program of its kind10. By the end of \\n1980s, many prominent institutions started academic programs/ departments in CS. However, in the early \\nperiod, the major focus was on education, training and manpower development. Research activity in CS \\nstarted to grow only in 1990s. The last two decades have seen CS research gaining momentum, with \\nestablishment of CS departments in all major universities/ institutions and establ ishment of specialized \\ninstitutions for Information Technology (IT) teaching and research. Some of these new specialized IT \\n                                                           \\n2 http://en.wikipedia.org/wiki/Computer_science  \\n3 http://cs.stanford.edu/  \\n4 http://www.cse.iitk.ac.in/dept/index.html  \\n5 http://www.cse.iitm.ac.in/  \\n6 http://cse.iitkgp.ac.in/  \\n7 http://www.cse.iitb.ac.in/page17  \\n8 http://www.csa.iisc.ernet.in/aboutus/aboutus.php  \\n9 http://www.jaduniv.edu.in/htdocs/view_department.php?deptid=59  \\n10 http://www.isical.ac.in/~mtechcs/  \", metadata={'source': './CS-Research.pdf', 'page': 5}),\n",
       " Document(page_content='6 \\n \\ninstitutions have gained a very respectable presence in CS research in a very short span of time. Some of the \\nmajor IT companies hav e also started their research laboratories in India during the last 2 -3 decades, such as \\nIBM Research in 1988 and Microsoft Research in 2005. Some applied research in CS domain is now also \\ncarried out at various government laboratories/ organizations, such  as CSIR11, DRDO12, ISRO13 etc. \\n \\nIn order to have a more informed picture of the current status of CS research in India, we have collected \\ndata for  CS domain research output of all institutions in India , for the 25 year period from 1989 to 2013 , \\nfrom the Scop us database . This period happens to be the period of actual growth in CS research in India. \\nWe plot  the year-wise proportionate contribution of India  to the total CS domain research worldwide in \\nFigure 1 . We see that India’s share to the total CS research output of the world is increasing in general, \\nexcept in few years . In terms of percentage contribution, it increased from 1.88% in 1989 to 4.95% in 2013 . \\nThe rate of growth is more clearly visible from 2000-01. Also, computing the rates of growth of CS \\nresearch in India and the total world tells us that India ’s average rate of growth is higher than the world . \\n \\n4. Data Collection  \\nWe have collected data of CS research output for a period of 25 years (1989 -2013)  from Scopus, a well -\\nknown bibliometric database . A total of 2 ,876,512 publication records are (were)  found for the period 1989 -\\n2013  for the whole world , where India contributes 84,385 ( about 2.93%) research papers , as on March \\n2014 . We collected data ins titution -wise for 100 most productive institutions from  India (referred to as \\nI100) and the world  (referred to as W 100). Our collection, though  not exhaustive , is a good representative \\nof research output data . The collected representative data covers 846,527 846,526 records ( about 29.43%  \\n29.6%) of the world CS research output  and 59,682 59,619  records ( 70.73%  71%) of the total Indian CS  \\nresearch output . Out of a total of 61,502 records in Scopus for  Indian CS  research output  (I100) , 1,314 \\nentries  have missing year value and about 506 duplicate entries , which leave  unique records as 59,682. Out \\nof a total 60,119 records in Scopus for Indian CS research output (I100), duplicate records were removed, \\nwhich leave 59 ,619 unique records. Similarly, for world universities  in CS research (W100), 846,527  \\n846,526  are unique records as against 924,575 records originally found in data .  In addition to CS domain \\nresearch output of I100 and W100, we also collected some related statistics for India and world, such as \\n                                                           \\n11 http://www.csir.res.in/  \\n12 www.drdo.gov.in  \\n13 http://www.isro.org/  ', metadata={'source': './CS-Research.pdf', 'page': 6}),\n",
       " Document(page_content='7 \\n \\nGross Expenditure on Research and Development (GERD), Researchers per million people etc. The table 1  \\nshows these values to establish a broader background perspective for our analytical characterization.  \\n \\n5. Methodology  \\nOur analytical characterization methodology is primarily a two -dimensional ap proach, comprising of a \\nstandard scientometric analysis and a computational text analysis. First, w e perform a scientometric \\ncharacterization along the six main indicators, namely Total Publication (TP), Total C itations (TC), \\nAverage Citation Per Paper (ACPP), HiCP (Highly Cited Papers), Internationally Collaborated P apers (ICP) \\nand the cited percentage . Out of these, o nly TP i s a primary indicator, and all other secondary indicators are \\ncomputed programmatically  from the data.  We have computed these indicators for all the institutions in \\nboth the sets. Second major part of our analysis is to characterize the major research themes pursued and \\nadvanced by the institutions in the set. For this task, we have processe d the entire data to classify every \\nresearch output in one (or more) of the 11 major thematic areas in CS research, as illustrated in Appendix. \\nThis thematic mapping is more informative than the subcategory labels used by the Scopus database  and is \\nextende d from a recent previous work (Uddin & Singh, 2015) . For classifying a paper to belong to a \\nthematic area, its ‘author keyword’, ‘paper title’ and ‘abstract’ fields are analyzed. A paper is assigned a \\nthematic class which has substantial match of keywords with the predefined thematic classes. After \\nattributing thematic areas to each paper, we measured the thematic area strength of all the institutions and \\nidentified leading institutions in a particular thematic area  (Thematic trend, we have done for I100 &  W100 \\nas a whole but not institution -wise). This has been done for the two sets, I100 and W100. While, the \\nscientometric analysis helps us in analytical and comparative characterization of research competitiveness \\nof I100 and W100 sets, t he text -based comp utational analysis helps us in identifying leading research \\nthemes pursued by institutions in the two sets and the similarities and differences  among them .   \\n \\n6. Scientometric Indicator Computations  \\nWe would first present the computational results of analytic al characterization using scientometric \\nindicators. The results compare and characterize the two sets, I100 and W100, along different indicators as \\nsummarized in the four subsections below.   \\n \\n6.1 Research Output  ', metadata={'source': './CS-Research.pdf', 'page': 7}),\n",
       " Document(page_content='8 \\n \\nWe first tried to do a detailed indicator -based comparison of institutions in I 100 and W100. Before, we \\npresent the results;  it would be relevant to mention here that no Indian institution figures in the W100 set of \\n100 most productive institutions worldwide . The relative rank on product ivity of Indian institutions can be \\nunderstood from the fact that total 25 year period CS research output of 100th institution (University College \\nLondon) in W100 is 5,747 whereas , the output of the two top ranking Indian institutions, namely Indian \\nInstitute of Technology, Kharagpur and Indian Institute of Science, Bangalore are 3,514 and 3,506, \\nrespectively. The most productive Indian institution has output levels below 150th rank institution in W100. \\nIt may also be worth mention ing that in a  recently published ranking by ARWU14, only two Indian \\ninstitutions, “Indian Statistical Institute” and “Indian Institute of Science”, are placed in the top 200 list, \\nunder  CS subject category.  We present the detailed statistics of scientometric indicator -based comparative \\nassessment in tables 2 and 3. The table 2 present s comparative figures for the sets, I100 and W100, as a \\nwhole. Form the tables; w e observe that there is a significant difference among indicators like ACPP (I100 \\naverage being 2.66 and W100 average 9.37), HiCP (average value of 60 for I100 and 855 for W100), ICP \\n(110 for I100 as against 2419 for W100) and cited percentage (39.33% for I100 and 58.91% for W100). \\nThe table 3 presents values of various scien tometric indicators for 20 most productive institutions in I100 \\nand W100, for a more detailed comparison.    \\n \\n6.2 Co -authorship Pattern s \\nThe second important aspect we addressed is about the c o-authorship and collaboration patterns seen in the \\nCS research in top Indian institutions and the top institutions worldwide. The authorship pattern for research \\npapers has known to have effect on their overall impact. A paper with multiple (more than one) authors is \\nthe joint contribution of two or more authors  and has inputs from different people/ groups. Higher multi -\\nauthored papers are an indicator for higher interdisciplinary research and/ or higher level of collaboration \\nbetween researchers at national or interna tional level. Research projects involving bigger groups also result \\nin higher multi -authored papers. From the data, we observed the co-authorship  pattern s for India, and found \\nthat about 39.4% of the total CS research outputs of I100 involve two authors and about 32% with three \\nauthors. Only  about 5% of the output is by a single author , indicating that about 95% of the total CS output \\ninvolves some form of collaboration. The co -authorship pattern for the W100 , however, shows different \\nscenario  where the co -authorship distribution is less skewed (0.91) than that of Indian CS co -authorship \\n                                                           \\n14 http://www.shanghairanking.com/SubjectCS2014.html  ', metadata={'source': './CS-Research.pdf', 'page': 8}),\n",
       " Document(page_content='9 \\n \\ndistribution (1.51). The Figure s 2(a) and 2(b) show year -wise trends of co -authorship patterns f or the \\ncollected data. There is a clear trend towards higher co llaboration between researchers, as measured by \\nmultiple authors in more and more research papers. In the most recent year, about 50% of the total CS \\nresearch for W100  is with three or more authors,  while it is about 30% for the I100.   \\n \\n6.3 International Collaboration Patterns  \\nAfter observing a higher proportion of multi -authored papers, we aimed to measure the types of \\ncollaboration patterns as national or international. Higher international collaboration is not only an indicator \\nof multi -national collabo ration but also shows maturity and international nature of a discipline. A study  \\n(Smith 1958)  was perhaps the first work on collaboration and it propose d that collaborative papers can be \\nused as a proxy measure of collaboration  among researchers.  In another work, Glanzel  (2001)  show ed that \\ninternational co -authorship, on average, results  in publications with higher citation rates than purely \\ndomestic papers. Through computational analysis, we found  that 95.5%  of the I100 are co-authored  (with \\naverage co -authorship value as 2.92) , whereas 93.5%  of the W100  are co-authored  (but average co -\\nauthorship value as 3.41) . The average co -authorship for W100  is higher, an indicator of higher level of \\ncollaboration . The Figure 3  shows collaboration patterns for outputs of I100 and W100 . We observe that \\nI100 has 25% of the collaborated papers involving international collaboration as against 30% value for \\nW100 . We also identified the top collaborating countries by Indian authors. The Table 2 shows the list of \\ntop 20 countries  having highest collaboration in I100 output during the 25 year period. The United States  of \\nAmerica  contributes largest share of I100’s international collaboration ( with 4316  research papers) followed \\nby Singapore  (849 research papers) and  Canada ( 801 research papers ). The W100  data shows higher \\ninternational collaborative research papers, including multi -country collaboration outputs.  \\n \\n6.4 Citation -based Impact  Assessment  \\nAn independent report by Thompson Reuters showed that India had about 0.52% of its national output (all \\ndisciplines) in the worldwide HiCP list in 2011  (Thompson Reuters Report on The Research and Innovation \\nPerformance of G20, March 2014 ). It is well acce pted that s olely counting number of research publications \\nmay not be the best method  to measure the scientific research productivity  and usefulness . It focuses \\nprim arily on the output quantity and not on the quality as measured through impact/ productivity . It would \\nnaturally be unfair to treat two research papers, one with very high citation count and the other with low or ', metadata={'source': './CS-Research.pdf', 'page': 9}),\n",
       " Document(page_content='10 \\n \\nno citation, as equally useful. On the other hand, a research paper with higher citation is considered more \\neffective, useful and productive.    Citation count is considered an effective measure of productivity and \\ninfluence of a research paper. We have extracted total citations of each research paper in the I100 and \\nW100  data and also identif ied the top cited papers  (top 10% most cit ed papers)  in each group for each of the \\n25 years. We track the institution information for all such records and look for top contributing institutions \\nto the set of highly cited (top 10%) publications. The figure 4(a) shows the number of collaborated pape rs \\namong the top 10% highly cited papers  in I100. We find that t otal of 1797 papers (app roximately  30% \\namong the top 10% cited papers ) are collaborated internationally.  After an assessment of relationship \\nbetween collaboration and citation -impact, we moved to identify the top institutions in I100 with highest \\nproportions of top 10% highly cited papers  in I100. The  figure 4(b)  shows top five institutions for each year \\nchronologically. The vertical axis represents the contribution in percentage to the top10% cited papers in \\nthe particular year. The figure clearly shows that initially only certain selected institutions contributed to th e \\nelite set of papers and grabbed maximum of the set. However, o ver the period of time other institutions also \\ncontributed to the top 10% highly cited papers, an indicator of quality research output being generated from \\nmore and more institutions. On an av erage, 12 institutions from India are the most frequent  contribut ors to \\nthe top cited papers in I100 set. The Indian Institute of Science (IIS) showed almost omnipresence  in all the \\nyears , except 1989 , indicating most consistent qualitative output . The Indian Institute of Technology, \\nKharagpur (IITK) , Indian Institute of Technology, Delhi (IITD) and Indian Statistical Institute, Kolkata \\n(ISIK) are few other frequent names in the list. We also measured the proportion of CS research output in \\nI100 and W100  that remains un -cited. The figure 4(c) shows the percentage of cited and un -cited papers  for \\nboth I100 and W100 , shown  over five consecutive chronological blocks  of five years each. The problematic \\nindication observed is that the percentage of papers that are cited is decreasing over the time. This may be \\ndue to smaller citation window availability or possibly with higher amount research output, a large part of \\nwhich is not that useful/ productive. On a comparison between I100 and W100  data, we observe that the \\ncorresponding cited figures (in percentage) for first two blocks are almost same but the values for W100  in \\nrecent three blocks are higher, implying low  immediacy index  for CS research output from India . \\n \\n \\n7. Performance based on  Trajectory A nalysis  \\nA recent paper (Nishy 2012) proposed a n exergy rank based on a thermodynamic analogy . The exergy \\nindicator (referred as X) is a multiplicative product of quality and quantity of a scientist’s or research ', metadata={'source': './CS-Research.pdf', 'page': 10}),\n",
       " Document(page_content='11 \\n \\ngroup’s performance . The measure is derived from primary/ secondary indicators like impact, citations and \\nnumber of papers . The proposed system tries to define a single meaningful indicator that combines both, the \\nquantit y (number of papers published , P) and the quality (measured as impact , i=C/P, where C is citation \\ncount ) aspects. A quality proxy, called exergy is defined as X=(C/P)*C . The values of i and P or i and C \\ncan be shown as  trajector ies on two -dimensional plot with respect to  time (known as iPX and iCX \\napproaches, respectively) . Both the iCX (the contour lines for X=iC ) and iPX (the contour lines for X=i2P) \\ncan be chosen for two dimensional mapping of the information. The iPX approach is more intuitive as it \\nshows the direct relation between quantity ( P) and quality ( i) which is of our interest but in iCX approach \\noutput P is hidden and does not directly reflect the quantity ( P). We have used the exergy measure to \\ncompute quantity -quality composite rank for some selected institutions in I100 and W 100.  \\n \\nWe have measured th e research performance of the top 5 Indian institutions in I100 and plotted them with \\nthe top 5  institutions in W100  list based on their TP value . This is done  primarily to compare the relative \\nperformance of Indian institutions vis -à-vis the institutions in world top 100. Unfortunately, no Indian \\ninstitution appears in the W100  list. The figure 8(a) displays  the exergy values for the selected institutions . \\nHere, the X-axis represents the time in year and Y-axis denotes the obtained exerg y score in that particular \\nyear. Similarly, two sets of institutions were selected based on ACPP and on HiCP values. Selecting \\ninstitutions on the values of different indicators help us to see the performance variability of institutions  \\nleading on different indicators’ scale. Figures 8(b) and 8(c)  shows the performance in exergy for institutions \\nselected based on ACPP and HiCP, respectively.  All the plots are made f or eight 5 year rolling windows , \\nfrom 2001 -2005 to 2008 -2012  to see the pe rformance in the following 6th year (2006 -2013) . We see that all \\nselected Indian institutions ’ performances  are below all selected W100 institutions’ performances , a result \\nthat needs serious thought. Among the top Indian institutions  in I100, most of them  are IITs . All the exergy \\ngraphs show  that in terms of performance (quantity -quality) measure Indian institutions are far behind than \\nthat of W100  institutions. The important thing here to mention is that we did not consider the academic \\ninfrastructure suc h as number of faculties, active researchers etc in the context . The data and computed \\nvalues are shown in supplementary table 5(a-c). The supplementary table s show the paper published ( P) by \\nthe leading institutions over a five year publication window and  citations ( C) obtained in the next year for \\neach publication window. These values are used to compute the impact ( i=C/P ) and the exergy ( X=C2/P). \\n ', metadata={'source': './CS-Research.pdf', 'page': 11}),\n",
       " Document(page_content='12 \\n \\n8. Identifying Research T hemes in Output  \\nIn addition to the scientometric analysis, we also performed a text -based analysis of the data to identify \\nmajor research topics/ themes and their variations with respect to time. We used the burst detection \\nalgorithm (Kleinberg 2003)  for this task . We app lied the algorithm on the “indexed keyword ” field of our \\ndata set  for the entire 25 year period. The burst detection measures the keyword weights which is a function \\nof the occurrence  frequency and the time span of the keyword.  The Science of Science (Sci2 )15 was used for \\ntemporal analysis (Burst Detection).   A keyword with a heavy weight indicates that the topic was \\nresearched frequently for a significant period of time. The figure s 6(a) and 6(b) show keyword density \\nplots, based on their corresponding weight s, for I100 and W100  data, respectively. For the visualization \\npart, we used VOSviewer16 reflecting the weights of corresponding keyword s. We can see that “Computer \\nSimulation” and “Mathematical Models” are present in leading positions in both figures. Similarly \\n“Problem Solving” appear s in both plots . However, the W100  plot shows a significan ce of topics like  \\n“Algorithms”, “Approximation Theor y”, “Network Protocols” and “ Computational Complexity” ; where the \\nI100 plots show significance of topics like “Computer Networks”, “Information Technology”, \\n“Communication” and “Computational Methods” , for the entire 25 year period considered. In \\nsupplemen tary figures 6 (c ) to 6(g), we show the tag cloud plot visualization of the major research themes \\nfor the I -Top10  in the left and W100  in the right .   The tag -cloud for the su b-period 1989 -1993 shows that \\n“Algorithm”, “M athematical Models”, “Computer S imulation” were  the leading topics in I100, whereas \\nW100  had frequent occurrence of “Algorithm”, “Computer S imulation ”, “Optimization” and “Image \\nProcessing” . In the 1994 -1998  block , “Algorithms”, “Communication”, “O ptimizat ion”, “Information \\nTechnology” &  “Artificial I ntelligence” were the top topics  in I100, whereas  “Mathematical M odels” & \\n“Computer S imulation” dominated in W100 . In 1999 -2003, “Mathematical M odels” again took the leading \\nposition  followed by “A lgorithms  for I100 and “Mathematical Models ” & “Optimization ” dominated in \\nW100 . In 2004 -2008, “Mathematical M odels” became more  significant as compared to “Algorithm”  for \\nI100 and in W100 , “Problem S olving” & “Optimization” were prominent . In 2009 -2013, we can see a slight \\nchange from previous block’s trend s, as  “Computer Simulation” & “A lgorithms” appeared as major \\nresearch themes for I100, whereas W100  shows a variation including “O ptimization”, “ Artificial \\nIntelligence” & “Signal P rocessing” . In general, “Algorithm” , “Computer S imulation”  & “Mathematical \\n                                                           \\n15 https://sci2.cns.iu.edu/user/index.php  \\n16 http://www.vosviewer.com/  ', metadata={'source': './CS-Research.pdf', 'page': 12}),\n",
       " Document(page_content='13 \\n \\nModels”  are prominent research themes i n both I100 & W100  data. The W100  data also shows  \\n“optimization” as a frequent research t heme .   \\n \\n9. Summary and Conclusion  \\nThe paper  reports our results on scientometric, network -theoretic and text -based analysis of CS research in \\nmajor institutions in India ( I100 as representative sample) and in the world ( W100  as representative sample) \\nduring the last 25 year period (1989 -2013). The analytical results present  measurements the characteristic \\nsimilarities and differences between I100 and W100 showing primary and secondary indicators of CS \\nresearch , such as absolute and comparative growth , collaboration patterns, year-wise citation -based impact \\nassessment  etc. We have also att empted to show the contribution of Indian institutions to world CS output \\nand the impact of Indian CS output . The Quality -Quantity Exergy performance measures were shown and \\ndiscussed for top institutions from I100 and W100 . Finally , we report outcome of r esearch theme trend \\nmapping through a text -based analysis  of the data. A year -block wise temporal trend  identification of \\nresearch theme is done along with burst detection of CS research output data and visualized graphically.  \\n \\nThe results found indicate t hat India has a lot to enhance it research activities with quality. Although India \\nmaintained the same trends in CS research but the quality research is still lacking. The results report \\nanalysis of a substantial period of 25 years and can be used in many ways :  to take a look at CS research in \\nIndia vis -à-vis top ranking world institutions;  to formulate polic ies for promoting research and enhancing \\nquality output ; to help a student selecting an institution for his/her research ; and other academic/ scientific \\npurposes.   \\n \\nAcknowledgement  \\nThis work is supported by research grants from Department of Science and Technology, Government of India \\n(Grant: INT/MEXICO/P -13/2012) and University Grants Commission of India (Grant: F. No. 41 -624/ \\n2012(SR)).  \\n \\nReferences  \\nAbramo, G., D’Angelo, C.A.  & Murgia, G. (2013). The collaboration behaviors of scientists in Italy: A \\nfield level an alysis. Journal of Informetrics , 7(2), 442 -454.  \\nCosta, B.M.G., Pedro, E.S.  & Macedo, G.R. (2013). Scientific c ollaboration in biotechnology: the case of \\nthe northeast region in Brazil. Scientometrics, 95(2), 571 –592. ', metadata={'source': './CS-Research.pdf', 'page': 13}),\n",
       " Document(page_content='14 \\n \\nDe Souza, C.G. & Ferreira, M.L. A. (2012).  Researchers profile, co -authorship pattern and knowledge \\norganization in information science in Brazil. Scientometrics, 95(2), 673 –687. \\nFu, H.Z, & Ho, Y.S. (2013). Independent research of China in Science Citation Index Expanded during \\n1980 –2011. Journal of Informetrics, 7(1), 210 -222. \\nGlänzel, W. (2001). National characteristics in international scientific co -authorship relations. \\nScientometrics, 51(1), 69 –115. \\nGupta, B.M., Kshitij, A.  & Verma, C. (2011). Mapping of Indian computer science research output , 1999 –\\n2008. Scientometrics, 86(2), 261 –283. \\nKao, C., Liu, S. &  Pao, H. (2012). Assessing improvement in management research in Taiwan. \\nScientometrics, 92(1), 75 –87. \\nKleinberg, J. (2003). Bursty and Hi erarchical Structure in Streams.  Data Mining and Knowle dge Discovery, \\n7, 373 –397. \\nLiu, Y., Rousseau, R.  & Guns., R. (2013). A layered framework to study collaboration as a form of \\nknowledge sharing and diffusion. Journal of Informetrics, 7(3), 651 -664. \\nMa, R.., Ni, C.  & Q iu, J. (2008). Scientific research competitiveness of world universities in computer \\nscience. Scientometrics, 76(2), 245 –260. \\nMatthews, A.P. (2013). Physics publication productivity in South African universities. Scientometrics. \\n95(1), 69 –86. \\nNishy, P., Panwa r, Y., Prasad, S., Mandal, G.K.  & Prathap, G. (2012). An impact -citations -exergy (iCX) \\ntrajectory analysis of leading research institutions in India. Scientometrics, 91(1), 245 -251. \\nOnyancha O.B.  & Maluleka, J.R. (2011). Knowledge production through collaborative research in sub -\\nSaharan Africa: how much do countries contribute to each other’s knowledge output and citation impact?. \\nScientometrics, 87(2), 315 –336. \\nOrtega, J.L.  & Aguillo, I.F. (2013). Institutional and country collaboration in an online service of scientific \\nprofiles: Google  Scholar Citations. Journal of Informetrics, 7(2), 394 -403. \\nOzel, B. (2012). Collaboration structure and knowledge diffusion in Turkish management academia. \\nScientometrics, 93(1), 183 –206. \\nPrathap, G. (2013). Second order indicators for evaluating internat ional scientific collaboration. \\nScientometrics, 95(2), 563 –570.  \\nSmith, M. (1958). The trend toward s multiple authorship in psychology. American Psychologist, 13(10), \\n596–599. \\nTang, L.  & Shapira, P. (2011). China –US scientific collaboration in nanotechnolo gy: patterns and \\ndynamics. Scientometrics, 88(1), 1 –16. \\nTeodorescu, D.  & Andrei, T. (2011). The growth of international collaboration in East European scholarly \\ncommunities: a bibliometric analysis of journal articles published between 1989 and 2009. Scien tometrics, \\n89(2),  711–722. \\nUddin, A.  & Singh, V.K. (2014). Mapping the c ompute r science research in SAARC c ountries. IETE \\nTechnical Review, 31(4), 287-296.  \\nUddin, A.  & Singh, V.K. (2014).  Measuring research output and collaboration in South Asian countri es. \\nCurrent Science , 107(1), 31 -38. \\nUddin, A. & Singh, V.K. (2015). A Quantity -Quality Composite Ranking of Indian Institutions in \\nComputer Science Research. IETE Technical Review  (forthcoming) DOI: \\nhttp://dx.doi.org/10.1080/02564602.2015.1010614  ', metadata={'source': './CS-Research.pdf', 'page': 14}),\n",
       " Document(page_content='15 \\n \\nViana , M.P., Amancio, D .R. & Costa, L.F. (2013). On time -varying collaboration networks. Journal of \\nInformetrics, 7(2), 371 -378. \\nView publication stats', metadata={'source': './CS-Research.pdf', 'page': 15})]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e407e4a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rasha\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\langchain\\chains\\retrieval_qa\\base.py:206: UserWarning: `VectorDBQA` is deprecated - please use `from langchain.chains import RetrievalQA`\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the Question you want to ask from your model: What is the theme of this research paper?\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' This research paper is about analyzing the research output of Computer Science in major institutions in India and in the world in order to compare them and identify research themes and their trends.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_splitter = CharacterTextSplitter(chunk_size=2000, chunk_overlap=100)\n",
    "texts = text_splitter.split_documents(documents)\n",
    "embeddings = OpenAIEmbeddings(openai_api_key='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')\n",
    "docsearch = Chroma.from_documents(texts, embeddings)\n",
    "\n",
    "from langchain.chains import RetrievalQA\n",
    "qa = VectorDBQA.from_chain_type(llm=OpenAI(), chain_type=\"stuff\", vectorstore=docsearch)\n",
    "\n",
    "documents\n",
    "\n",
    "query1 = input(\"Enter the Question you want to ask from your model: \")\n",
    "qa.run(query1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
